{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Swiggy FAQ data\n",
    "data = {\n",
    "    \"Question\": [\n",
    "        \"What is Swiggy Customer Care Number?\",\n",
    "        \"Can I edit my order?\",\n",
    "        \"I want to cancel my order\",\n",
    "        \"Will Swiggy be accountable for quality/quantity?\",\n",
    "        \"Is there a minimum order value?\",\n",
    "        \"Do you charge for delivery?\",\n",
    "        \"How long do you take to deliver?\",\n",
    "        \"What are your delivery hours?\",\n",
    "        \"Can I order from any location?\",\n",
    "        \"Is single order from many restaurants possible?\",\n",
    "        \"Do you support bulk orders?\",\n",
    "        \"Can I order in advance?\",\n",
    "        \"Can I change the address/number?\",\n",
    "        \"Did not receive OTP?\",\n",
    "        \"Did not receive referral coupon?\",\n",
    "        \"Deactivate my account\",\n",
    "        \"Unable to view the details in my profile\",\n",
    "        \"What is Swiggy Money?\",\n",
    "        \"Do you accept Sodexo, Ticket Restaurant etc.?\"\n",
    "    ],\n",
    "    \"Answer\": [\n",
    "        \"You can reach Swiggy's customer care at 080-67466729.\",\n",
    "        \"Your order can be edited before it reaches the restaurant. You could contact the customer support team via chat or call to do so. Once the order is placed and the restaurant starts preparing your food, you may not edit its contents.\",\n",
    "        \"We will do our best to accommodate your request if the order is not placed to the restaurant. Please note that we will have the right to charge a cancellation fee up to the full order value to compensate our restaurant and delivery partners if your order has been confirmed.\",\n",
    "        \"Quantity and quality of the food are the restaurant's responsibility. However, in case of issues with quality or quantity, kindly submit your feedback, and we will pass it on to the restaurant.\",\n",
    "        \"We have no minimum order value, and you can order for any amount.\",\n",
    "        \"Delivery fees vary from city to city and are applicable if the order value is below a certain amount. Additionally, certain restaurants might have fixed delivery fees. Delivery fees (if any) are specified on the 'Review Order' page.\",\n",
    "        \"Standard delivery times vary by the location selected and prevailing conditions. Once you select your location, an estimated delivery time is mentioned for each restaurant.\",\n",
    "        \"Our delivery hours vary for different locations and depend on the availability of supply from restaurant partners.\",\n",
    "        \"We will deliver from any restaurant listed in the search results for your location. We recommend enabling your GPS location finder and letting the app auto-detect your location.\",\n",
    "        \"We currently do not support this functionality. However, you can place orders for individual items from different restaurants.\",\n",
    "        \"In order to provide all customers with a great selection and to ensure on-time delivery of your meal, we reserve the right to limit the quantities depending on supply.\",\n",
    "        \"We currently do not support this functionality. All our orders are placed and executed on-demand.\",\n",
    "        \"Any major change in the delivery address is not possible after you have placed an order with us. However, slight modifications like changing the flat number, street name, landmark, etc. are allowed. If you have received delivery executive details, you can directly call him; else you could contact our customer service team.\",\n",
    "        \"Please check if your app is due for an update. If not, please share the details via support@swiggy.in.\",\n",
    "        \"Referral coupon is given upon the first successful transaction of the referred person. If you still have not received it, kindly send us your details at support@swiggy.in. We will contact you within 48 hours.\",\n",
    "        \"Please write to us at support@swiggy.in in the event that you want to deactivate your account.\",\n",
    "        \"Please check if your app is due for an update. If not, please share the details via support@swiggy.in.\",\n",
    "        \"Swiggy Money is a secure digital wallet where you can store digital currency and use it for faster checkouts. It prevents payment failures and gives you seamless refunds when necessary.\",\n",
    "        \"We do not accept Sodexo vouchers, but we do accept Sodexo cards. You can select the Sodexo card option while selecting payment options at the time of order.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "faq_df = pd.DataFrame(data)\n",
    "# faq_df.to_csv('faq.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "# faq_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy<2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linge\\Documents\\swiggy chatbot\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Embed the questions\n",
    "faq_df['Embeddings'] = faq_df['Question'].apply(lambda x: get_embeddings(x).numpy())\n",
    "\n",
    "# # Display the DataFrame with embeddings\n",
    "# faq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"f7f2e9e3-c8c8-4559-8721-edfa5e8901a3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --quiet pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from pinecone import ServerlessSpec, Pinecone\n",
    "\n",
    "# Initialize Pinecone by creating an instance of the Pinecone class\n",
    "pc = Pinecone(api_key=api_key)\n",
    "\n",
    "index_name = \"chatbot\"\n",
    "\n",
    "# Check if the index exists, and if not, create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=19,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=\"f7f2e9e3-c8c8-4559-8721-edfa5e8901a3\")\n",
    "index = pc.Index(\"chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'faq_1',\n",
      "              'metadata': {'question': 'Can I edit my order?'},\n",
      "              'score': 0.99916935,\n",
      "              'values': [-0.334157169,\n",
      "                         0.122185685,\n",
      "                         0.463736922,\n",
      "                         0.242123455,\n",
      "                         -0.252664536,\n",
      "                         0.764290631,\n",
      "                         -0.523814082,\n",
      "                         -0.387417465,\n",
      "                         0.150341257,\n",
      "                         0.414427161,\n",
      "                         -0.0870244652,\n",
      "                         0.9433,\n",
      "                         -0.235966578,\n",
      "                         -0.633038,\n",
      "                         -0.200092703,\n",
      "                         -0.437591732,\n",
      "                         0.140113026,\n",
      "                         0.32617268,\n",
      "                         -0.850405216]}],\n",
      " 'namespace': 'faq_namespace',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "# Prepare vectors for upsert\n",
    "vectors_to_upsert = [\n",
    "    {\n",
    "        \"id\": f\"faq_{i}\",\n",
    "        \"values\": faq_df['Embeddings'][i].flatten().tolist()[:19],  # Ensure the vector is 19-dimensional\n",
    "        \"metadata\": {\"question\": faq_df['Question'][i]}\n",
    "    }\n",
    "    for i in range(len(faq_df))\n",
    "]\n",
    "\n",
    "# Upsert vectors into Pinecone\n",
    "index.upsert(\n",
    "    vectors=vectors_to_upsert,\n",
    "    namespace=\"faq_namespace\"\n",
    ")\n",
    "\n",
    "# Query example\n",
    "query_vector = get_embeddings(\"Can I edit my order?\").flatten().tolist()[:19]  # Match the 19 dimensions\n",
    "results = index.query(\n",
    "    namespace=\"faq_namespace\",\n",
    "    vector=query_vector,\n",
    "    top_k=1,\n",
    "    include_values=True,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\linge\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from langchain.llms import HuggingFaceEndpoint  # Assuming you're using LangChain\n",
    "\n",
    "# Set your Hugging Face token\n",
    "sec_key = \"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\"\n",
    "HUGGINGFACEHUB_TOKEN=sec_key\n",
    "# Log in using your Hugging Face token\n",
    "login(token=sec_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can reach Swiggy's customer care at 080-67466729. \n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = \"<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>\"\n",
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "# Hugging Face model setup\n",
    "repo_id = \"google/flan-t5-large\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Function to call the LLM with improved control over the response\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"temperature\": 0.7,  # Balanced creativity to maintain factual accuracy\n",
    "                \"top_p\": 0.9,  # Ensures varied output but controls randomness\n",
    "            },\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "# Function to find a relevant FAQ from the database with confidence threshold\n",
    "def find_relevant_faq(query, threshold=0.8):\n",
    "    query_vector = get_embeddings(query).flatten().tolist()[:19]  # Match the 19 dimensions\n",
    "    results = index.query(\n",
    "        namespace=\"faq_namespace\",\n",
    "        vector=query_vector,\n",
    "        top_k=1,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    if results[\"matches\"]:\n",
    "        best_match = results[\"matches\"][0]\n",
    "        if best_match[\"score\"] >= threshold:  # Check if the match confidence is above the threshold\n",
    "            question = best_match[\"metadata\"][\"question\"]\n",
    "            answer = faq_df[faq_df[\"Question\"] == question][\"Answer\"].values[0]\n",
    "            return question, answer\n",
    "    return None, None\n",
    "\n",
    "# Function to generate a response based on the user query with emphasis on accuracy\n",
    "def generate_response(user_query):\n",
    "    question, answer = find_relevant_faq(user_query)\n",
    "\n",
    "    if question and answer:\n",
    "        # Construct a prompt for the LLM that encourages creativity but emphasizes factual accuracy\n",
    "        prompt = (\n",
    "            f\"The user asked: '{user_query}'.\\n\\n\"\n",
    "            f\"Relevant FAQ:\\nQ: '{question}'\\nA: {answer}\\n\\n\"\n",
    "            f\"Please use this FAQ as the basis of your response, but feel free to elaborate creatively to engage the user.\"\n",
    "            f\" Make sure your response is informative and accurate, but can also provide some context or extra suggestions.\"\n",
    "        )\n",
    "        response = call_llm(llm_client, prompt)\n",
    "\n",
    "        # Double-check that the response contains relevant information from the FAQ\n",
    "        if answer.lower() in response.lower():  # Ensure that the core answer is present\n",
    "            return response.strip()\n",
    "        else:\n",
    "            # Fall back to the FAQ answer if the model strays too far\n",
    "            return f\"{answer} \"\n",
    "    else:\n",
    "        # Return a fallback response when no relevant FAQ is found\n",
    "        return (\n",
    "            \"I couldn't find an exact match for your question, but I'm here to help! \"\n",
    "            \"Could you please provide more details, or try asking in another way?\"\n",
    "        )\n",
    "\n",
    "# Example usage\n",
    "query = \"what is the swiggy customer care number?\"\n",
    "response = generate_response(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\linge\\Documents\\swiggy chatbot\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\linge\\.cache\\huggingface\\token\n",
      "Login successful\n",
      "You can reach Swiggy's customer care at 080-67466729. \n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Swiggy FAQ data\n",
    "data = {\n",
    "    \"Question\": [\n",
    "        \"What is Swiggy Customer Care Number?\",\n",
    "        \"Can I edit my order?\",\n",
    "        \"I want to cancel my order\",\n",
    "        \"Will Swiggy be accountable for quality/quantity?\",\n",
    "        \"Is there a minimum order value?\",\n",
    "        \"Do you charge for delivery?\",\n",
    "        \"How long do you take to deliver?\",\n",
    "        \"What are your delivery hours?\",\n",
    "        \"Can I order from any location?\",\n",
    "        \"Is single order from many restaurants possible?\",\n",
    "        \"Do you support bulk orders?\",\n",
    "        \"Can I order in advance?\",\n",
    "        \"Can I change the address/number?\",\n",
    "        \"Did not receive OTP?\",\n",
    "        \"Did not receive referral coupon?\",\n",
    "        \"Deactivate my account\",\n",
    "        \"Unable to view the details in my profile\",\n",
    "        \"What is Swiggy Money?\",\n",
    "        \"Do you accept Sodexo, Ticket Restaurant etc.?\"\n",
    "    ],\n",
    "    \"Answer\": [\n",
    "        \"You can reach Swiggy's customer care at 080-67466729.\",\n",
    "        \"Your order can be edited before it reaches the restaurant. You could contact the customer support team via chat or call to do so. Once the order is placed and the restaurant starts preparing your food, you may not edit its contents.\",\n",
    "        \"We will do our best to accommodate your request if the order is not placed to the restaurant. Please note that we will have the right to charge a cancellation fee up to the full order value to compensate our restaurant and delivery partners if your order has been confirmed.\",\n",
    "        \"Quantity and quality of the food are the restaurant's responsibility. However, in case of issues with quality or quantity, kindly submit your feedback, and we will pass it on to the restaurant.\",\n",
    "        \"We have no minimum order value, and you can order for any amount.\",\n",
    "        \"Delivery fees vary from city to city and are applicable if the order value is below a certain amount. Additionally, certain restaurants might have fixed delivery fees. Delivery fees (if any) are specified on the 'Review Order' page.\",\n",
    "        \"Standard delivery times vary by the location selected and prevailing conditions. Once you select your location, an estimated delivery time is mentioned for each restaurant.\",\n",
    "        \"Our delivery hours vary for different locations and depend on the availability of supply from restaurant partners.\",\n",
    "        \"We will deliver from any restaurant listed in the search results for your location. We recommend enabling your GPS location finder and letting the app auto-detect your location.\",\n",
    "        \"We currently do not support this functionality. However, you can place orders for individual items from different restaurants.\",\n",
    "        \"In order to provide all customers with a great selection and to ensure on-time delivery of your meal, we reserve the right to limit the quantities depending on supply.\",\n",
    "        \"We currently do not support this functionality. All our orders are placed and executed on-demand.\",\n",
    "        \"Any major change in the delivery address is not possible after you have placed an order with us. However, slight modifications like changing the flat number, street name, landmark, etc. are allowed. If you have received delivery executive details, you can directly call him; else you could contact our customer service team.\",\n",
    "        \"Please check if your app is due for an update. If not, please share the details via support@swiggy.in.\",\n",
    "        \"Referral coupon is given upon the first successful transaction of the referred person. If you still have not received it, kindly send us your details at support@swiggy.in. We will contact you within 48 hours.\",\n",
    "        \"Please write to us at support@swiggy.in in the event that you want to deactivate your account.\",\n",
    "        \"Please check if your app is due for an update. If not, please share the details via support@swiggy.in.\",\n",
    "        \"Swiggy Money is a secure digital wallet where you can store digital currency and use it for faster checkouts. It prevents payment failures and gives you seamless refunds when necessary.\",\n",
    "        \"We do not accept Sodexo vouchers, but we do accept Sodexo cards. You can select the Sodexo card option while selecting payment options at the time of order.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "faq_df = pd.DataFrame(data)\n",
    "# faq_df.to_csv('faq.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "# faq_df.head()\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings\n",
    "\n",
    "# Embed the questions\n",
    "faq_df['Embeddings'] = faq_df['Question'].apply(lambda x: get_embeddings(x).numpy())\n",
    "\n",
    "# # Display the DataFrame with embeddings\n",
    "# faq_df.head()\n",
    "\n",
    "\n",
    "api_key=\"f7f2e9e3-c8c8-4559-8721-edfa5e8901a3\"\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "from pinecone import ServerlessSpec, Pinecone\n",
    "\n",
    "# Initialize Pinecone by creating an instance of the Pinecone class\n",
    "pc = Pinecone(api_key=api_key)\n",
    "index_name = \"chatbot\"\n",
    "\n",
    "# Check if the index exists, and if not, create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=19,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "pc = Pinecone(api_key=\"f7f2e9e3-c8c8-4559-8721-edfa5e8901a3\")\n",
    "index = pc.Index(\"chatbot\")\n",
    "\n",
    "# Prepare vectors for upsert\n",
    "vectors_to_upsert = [\n",
    "    {\n",
    "        \"id\": f\"faq_{i}\",\n",
    "        \"values\": faq_df['Embeddings'][i].flatten().tolist()[:19],  # Ensure the vector is 19-dimensional\n",
    "        \"metadata\": {\"question\": faq_df['Question'][i]}\n",
    "    }\n",
    "    for i in range(len(faq_df))\n",
    "]\n",
    "\n",
    "# Upsert vectors into Pinecone\n",
    "index.upsert(\n",
    "    vectors=vectors_to_upsert,\n",
    "    namespace=\"faq_namespace\"\n",
    ")\n",
    "\n",
    "# Query example\n",
    "query_vector = get_embeddings(\"Can I edit my order?\").flatten().tolist()[:19]  # Match the 19 dimensions\n",
    "results = index.query(\n",
    "    namespace=\"faq_namespace\",\n",
    "    vector=query_vector,\n",
    "    top_k=1,\n",
    "    include_values=True,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from langchain.llms import HuggingFaceEndpoint  # Assuming you're using LangChain\n",
    "\n",
    "# Set your Hugging Face token\n",
    "sec_key = \"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\"\n",
    "HUGGINGFACEHUB_TOKEN=sec_key\n",
    "# Log in using your Hugging Face token\n",
    "login(token=sec_key)\n",
    "\n",
    "HF_TOKEN = \"<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>\"\n",
    "from huggingface_hub import InferenceClient\n",
    "import json\n",
    "\n",
    "# Hugging Face model setup\n",
    "repo_id = \"google/flan-t5-large\"\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=repo_id,\n",
    "    timeout=120,\n",
    ")\n",
    "\n",
    "# Function to call the LLM with improved control over the response\n",
    "def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "    response = inference_client.post(\n",
    "        json={\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_new_tokens\": 200,\n",
    "                \"temperature\": 0.7,  # Balanced creativity to maintain factual accuracy\n",
    "                \"top_p\": 0.9,  # Ensures varied output but controls randomness\n",
    "            },\n",
    "            \"task\": \"text-generation\",\n",
    "        },\n",
    "    )\n",
    "    return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "# Function to find a relevant FAQ from the database with confidence threshold\n",
    "def find_relevant_faq(query, threshold=0.8):\n",
    "    query_vector = get_embeddings(query).flatten().tolist()[:19]  # Match the 19 dimensions\n",
    "    results = index.query(\n",
    "        namespace=\"faq_namespace\",\n",
    "        vector=query_vector,\n",
    "        top_k=1,\n",
    "        include_values=True,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    if results[\"matches\"]:\n",
    "        best_match = results[\"matches\"][0]\n",
    "        if best_match[\"score\"] >= threshold:  # Check if the match confidence is above the threshold\n",
    "            question = best_match[\"metadata\"][\"question\"]\n",
    "            answer = faq_df[faq_df[\"Question\"] == question][\"Answer\"].values[0]\n",
    "            return question, answer\n",
    "    return None, None\n",
    "\n",
    "# Function to generate a response based on the user query with emphasis on accuracy\n",
    "def generate_response(user_query):\n",
    "    question, answer = find_relevant_faq(user_query)\n",
    "\n",
    "    if question and answer:\n",
    "        # Construct a prompt for the LLM that encourages creativity but emphasizes factual accuracy\n",
    "        prompt = (\n",
    "            f\"The user asked: '{user_query}'.\\n\\n\"\n",
    "            f\"Relevant FAQ:\\nQ: '{question}'\\nA: {answer}\\n\\n\"\n",
    "            f\"Please use this FAQ as the basis of your response, but feel free to elaborate creatively to engage the user.\"\n",
    "            f\" Make sure your response is informative and accurate, but can also provide some context or extra suggestions.\"\n",
    "        )\n",
    "        response = call_llm(llm_client, prompt)\n",
    "\n",
    "        # Double-check that the response contains relevant information from the FAQ\n",
    "        if answer.lower() in response.lower():  # Ensure that the core answer is present\n",
    "            return response.strip()\n",
    "        else:\n",
    "            # Fall back to the FAQ answer if the model strays too far\n",
    "            return f\"{answer} \"\n",
    "    else:\n",
    "        # Return a fallback response when no relevant FAQ is found\n",
    "        return (\n",
    "            \"I couldn't find an exact match for your question, but I'm here to help! \"\n",
    "            \"Could you please provide more details, or try asking in another way?\"\n",
    "        )\n",
    "\n",
    "# Example usage\n",
    "query = \"what is the swiggy customer care number?\"\n",
    "response = generate_response(query)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
